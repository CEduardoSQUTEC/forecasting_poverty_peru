{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.Resize(244),\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download Data from open datasets.\n",
    "data = {\n",
    "    split: datasets.CIFAR10(\n",
    "        root='../dataset/',\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    ) for split in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121CIFAR10(L.LightningModule):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super(DenseNet121CIFAR10, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        densenet121 = models.densenet121()\n",
    "        densenet121.classifier = nn.Linear(\n",
    "            densenet121.classifier.in_features, 10\n",
    "        )\n",
    "        self.model = densenet121\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=data['train'], batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=data['val'], batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet121CIFAR10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    devices='devices',\n",
    "    accelerator='auto',\n",
    "    log_every_n_steps=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "Batch size 16 failed, trying batch size 8\n",
      "Finished batch size finder, will continue with full run using batch size 8\n",
      "Restoring states from the checkpoint path at /home/ceduardosq/Code/2-Projects/1-ForecastingPoverty/poverty_forecasting_peru/.scale_batch_size_b85942b2-d381-449a-baed-2cbe4268ca8e.ckpt\n",
      "Restored all states from the checkpoint at /home/ceduardosq/Code/2-Projects/1-ForecastingPoverty/poverty_forecasting_peru/.scale_batch_size_b85942b2-d381-449a-baed-2cbe4268ca8e.ckpt\n"
     ]
    }
   ],
   "source": [
    "if trainer.num_devices == 1:\n",
    "    tuner = Tuner(trainer)\n",
    "    tuner.scale_batch_size(model, mode=\"power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
