{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_directory_path = Path.cwd() / 'records'\n",
    "\n",
    "weights_file_path = records_directory_path / 'weights.pth'\n",
    "loss_directory_path = records_directory_path / 'loss'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download Data from open datasets.\n",
    "data = {\n",
    "    split: datasets.CIFAR10(\n",
    "        root='../dataset/',\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    ) for split in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataloaders\n",
    "dataloaders = {\n",
    "    split: DataLoader(\n",
    "        dataset=data[split],\n",
    "        batch_size=2 ** 10,\n",
    "        num_workers=4,\n",
    "    ) for split in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'plane',\n",
    "    'car',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetCIFAR10(nn.Module):\n",
    "    def __init__(self, n_classes = 10):\n",
    "        super(AlexNetCIFAR10, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 3 * 3, 4096), \n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(4096, n_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNetCIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights successfully recovered.\n"
     ]
    }
   ],
   "source": [
    "if weights_file_path.exists():\n",
    "    weights = torch.load(weights_file_path, weights_only=True)\n",
    "    model.load_state_dict(weights)\n",
    "    print(\"Model weights successfully recovered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hardware Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, dataloaders, device, epochs, optimizer, criterion):\n",
    "    since = datetime.now()\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for split in ['train', 'val']:\n",
    "            model.train() if split == 'train' else model.eval()\n",
    "\n",
    "            running_loss = .0\n",
    "\n",
    "            epoch_prefix = 'Training' if split == 'train' else 'Validation'\n",
    "            prefix = f'{epoch_prefix:10} Epoch {epoch + 1:2}/{epochs}'\n",
    "\n",
    "            for x, y in tqdm(dataloaders[split], desc=prefix):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(split == 'train'):\n",
    "                    pred = model(x)\n",
    "                    loss = criterion(pred, y)\n",
    "                    writer.add_scalar('Loss/' + split, loss, epoch)\n",
    "\n",
    "                if split == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    time_elapse = datetime.now() - since\n",
    "    print(f'Training complete in {time_elapse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   Epoch  1/5: 100%|██████████| 49/49 [01:31<00:00,  1.87s/it]\n",
      "Validation Epoch  1/5: 100%|██████████| 49/49 [00:29<00:00,  1.67it/s]\n",
      "Training   Epoch  2/5: 100%|██████████| 49/49 [01:43<00:00,  2.12s/it]\n",
      "Validation Epoch  2/5: 100%|██████████| 49/49 [00:37<00:00,  1.31it/s]\n",
      "Training   Epoch  3/5: 100%|██████████| 49/49 [01:44<00:00,  2.13s/it]\n",
      "Validation Epoch  3/5: 100%|██████████| 49/49 [00:35<00:00,  1.37it/s]\n",
      "Training   Epoch  4/5: 100%|██████████| 49/49 [01:52<00:00,  2.30s/it]\n",
      "Validation Epoch  4/5: 100%|██████████| 49/49 [00:41<00:00,  1.18it/s]\n",
      "Training   Epoch  5/5: 100%|██████████| 49/49 [01:52<00:00,  2.31s/it]\n",
      "Validation Epoch  5/5: 100%|██████████| 49/49 [00:33<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete in 0:11:43.598763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training(\n",
    "    model=model,\n",
    "    dataloaders=dataloaders,\n",
    "    device=device,\n",
    "    epochs=5,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights successfully saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), weights_file_path)\n",
    "print(\"Model weights successfully saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
