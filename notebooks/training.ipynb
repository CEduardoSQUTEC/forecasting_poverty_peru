{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import LightningModule\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data from open datasets.\n",
    "data = {\n",
    "    split: datasets.CIFAR10(\n",
    "        root='../dataset/',\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    ) for split in ['train', 'val']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlexNetCIFAR10(LightningModule):\n",
    "    def __init__(self, batch_size=1):\n",
    "        super(AlexNetCIFAR10, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 3 * 3, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        y_hat = self.classifier(x)\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.SGD(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=data['train'], batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset=data['val'], batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log('val_loss', loss, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNetCIFAR10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs=5, log_every_n_steps=16)\n",
    "tuner = Tuner(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.scale_batch_size(model, mode=\"power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
